import os
import json
from typing import List, Dict, Any
from openai import OpenAI
from .vector_service import VectorService

class LLMService:
    def __init__(self):
        """Initialize LLM service with OpenAI"""
        self.vector_service = VectorService()
        # Initialize OpenAI client
        api_key = os.getenv('OPENAI_API_KEY')
    if api_key and api_key != "your_api_key_here":
           try:
            self.openai_client = OpenAI(api_key=api_key)
            self.llm_enabled = True
            print("✅ OpenAI client initialized")
        except Exception as e:
            print(f"⚠️  OpenAI failed: {e}")
            self.openai_client = None
            self.llm_enabled = False
    else:
            print("⚠️  No OpenAI API key, using mock responses")
            self.openai_client = None
            self.llm_enabled = False
            self.default_model = "gpt-3.5-turbo"
    
    def search_relevant_documents(self, question: str, max_docs: int = 3) -> List[Dict[str, Any]]:
        """
        Search for relevant documents using vector similarity search
        """
        try:
            results = self.vector_service.search_similar(question, max_docs)
            return results
        except Exception as e:
            print(f"Vector search failed: {e}")
            return []
    
    def call_openai(self, prompt: str, model: str = None) -> Dict[str, Any]:
        """
        Call OpenAI API with error handling and token tracking
        """
        try:
            model = model or self.default_model
            
            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that answers questions based on provided documents."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            # Extract response data
            result = {
                "answer": response.choices[0].message.content,
                "model": model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "success": True
            }
            
            return result
            
        except Exception as e:
            return {
                "answer": f"Error calling OpenAI API: {str(e)}",
                "model": model,
                "usage": None,
                "success": False,
                "error": str(e)
            }
    
    def generate_response(self, question: str, use_rag: bool = True, model: str = None) -> Dict[str, Any]:
        """
        Generate response using LLM with optional RAG
        """
        try:
            if use_rag:
                # Search relevant documents
                relevant_docs = self.search_relevant_documents(question)
                
                # Combine context
                context = "\n\n".join([
                    f"Document: {doc.get('source', 'Unknown')}\nContent: {doc.get('content', '')}"
                    for doc in relevant_docs
                ])
                
                # Create RAG prompt
                prompt = f"""Answer the question based on the following document content. If the documents don't contain relevant information, say so clearly.

Document Content:
{context}

Question: {question}

Instructions:
- Base your answer primarily on the provided documents
- If information is missing from the documents, state that clearly
- Cite which document(s) you're referencing when possible
- Be concise but comprehensive"""
                
            else:
                # Direct prompt without RAG
                prompt = f"""Answer the following question: {question}"""
            
            # Call OpenAI API
            llm_result = self.call_openai(prompt, model)
            
            # Prepare final response
            response = {
                "answer": llm_result["answer"],
                "sources": relevant_docs if use_rag else [],
                "prompt": prompt,
                "use_rag": use_rag,
                "model": llm_result.get("model"),
                "usage": llm_result.get("usage"),
                "success": llm_result.get("success", False)
            }
            
            if not llm_result.get("success"):
                response["error"] = llm_result.get("error")
            
            return response
            
        except Exception as e:
            return {
                "error": f"Error generating response: {str(e)}",
                "answer": None,
                "sources": [],
                "success": False
            }
    
    def process_query(self, query: str, use_rag: bool = True, model: str = None) -> Dict[str, Any]:
        """
        Main method to process user queries
        """
        return self.generate_response(query, use_rag, model)
