from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
import os
import sys
from typing import List, Dict, Any
from pydantic import BaseModel

# æ·»åŠ  app ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

# å°å…¥ä½ çš„æœå‹™
try:
    from app.services.llm_service import LLMService
    from app.services.document_parser import DocumentParser, chunk_document
    from app.services.vector_service import VectorService
    print("âœ… Successfully imported your RAG services!")
except ImportError as e:
    print(f"âš ï¸  Could not import services: {e}")
    print("Will use fallback implementations")
    LLMService = None
    DocumentParser = None
    VectorService = None

# Create FastAPI app
app = FastAPI(
    title="KnowledgeOps Platform",
    description="RAG-powered Knowledge Management System with Conversation Memory",
    version="2.1.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Serve static files (frontend)
if os.path.exists("frontend"):
    app.mount("/static", StaticFiles(directory="frontend"), name="static")

# Initialize services
llm_service = None
doc_parser = None
vector_service = None

@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    global llm_service, doc_parser, vector_service
    
    print("ğŸš€ Initializing KnowledgeOps Platform with Memory Support...")
    
    # Initialize LLM Service
    if LLMService:
        try:
            llm_service = LLMService()
            print("âœ… LLM Service initialized")
        except Exception as e:
            print(f"âš ï¸  LLM Service initialization failed: {e}")
    
    # Initialize Document Parser
    if DocumentParser:
        try:
            doc_parser = DocumentParser()
            print("âœ… Document Parser initialized")
        except Exception as e:
            print(f"âš ï¸  Document Parser initialization failed: {e}")
    
    # Initialize Vector Service
    if VectorService:
        try:
            vector_service = VectorService()
            print("âœ… Vector Service initialized")
        except Exception as e:
            print(f"âš ï¸  Vector Service initialization failed: {e}")
    
    print("ğŸ‰ KnowledgeOps Platform startup complete with Memory Support!")

# Request/Response models
class QueryRequest(BaseModel):
    question: str
    max_results: int = 5
    use_rag: bool = True
    context: List[Dict[str, str]] = []
    session_id: str = "default"

class QueryResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]] = []
    confidence: float = 0.0
    success: bool = True
    model: str = "integrated"

@app.get("/")
async def serve_frontend():
    """Serve frontend HTML file"""
    if os.path.exists('frontend/index.html'):
        return FileResponse('frontend/index.html')
    return {
        "message": "KnowledgeOps Platform API is running!",
        "version": "2.1.0",
        "status": "Ready for intelligent conversations with memory!"
    }

@app.get("/health")
async def health():
    """Enhanced health check with real service status"""
    try:
        # Get document count
        doc_count = 0
        storage_dirs = ["storage", "data"]
        for storage_dir in storage_dirs:
            if os.path.exists(storage_dir):
                doc_count += len([f for f in os.listdir(storage_dir) 
                                if f.lower().endswith(('.pdf', '.docx', '.txt'))])
        
        # Get vector count (if vector service is available)
        vector_count = 0
        if vector_service:
            try:
                stats = vector_service.get_stats()
                vector_count = stats.get('total_chunks', 0)
            except:
                vector_count = 0
        
        # Get service status
        service_status = {}
        if llm_service:
            service_status = llm_service.get_service_status()
        
        return {
            "status": "healthy",
            "service": "knowledgeops-platform",
            "version": "2.1.0",
            "documents_count": doc_count,
            "vectors_count": vector_count,
            "total_tokens": 0,
            "features": {
                "conversation_memory": True,
                "context_aware": True
            },
            "services": {
                "llm_service": llm_service is not None,
                "vector_service": vector_service is not None,
                "document_parser": doc_parser is not None,
                **service_status
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "documents_count": 0,
            "vectors_count": 0,
            "total_tokens": 0
        }

@app.post("/query")
async def query_for_frontend(request: dict):
    """
    Real RAG-powered query endpoint with conversation memory support
    """
    try:
        # Extract parameters
        question = request.get("question", "")
        max_results = request.get("max_results", 5)
        context = request.get("context", [])  # å°è©±æ­·å²
        session_id = request.get("session_id", "default")  # æœƒè©± ID
        
        if not question.strip():
            raise ValueError("Question cannot be empty")
        
        # å»ºç«‹åŒ…å«ä¸Šä¸‹æ–‡çš„å¢å¼·æŸ¥è©¢
        context_text = ""
        if context:
            # åªä½¿ç”¨æœ€è¿‘çš„å°è©±ä½œç‚ºä¸Šä¸‹æ–‡
            recent_context = context[-6:]  # æœ€è¿‘3è¼ªå°è©±
            for msg in recent_context:
                role = msg.get('role', 'user')
                content = msg.get('content', '')
                if content:
                    context_text += f"{role}: {content}\n"
        
        # è¨˜éŒ„ä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…æ³
        print(f"ğŸ“ Session: {session_id}")
        print(f"ğŸ“š Using {len(context)} context messages")
        if context_text:
            print(f"ğŸ’­ Context preview: {context_text[:100]}...")
        
        # Use real LLM service if available
        if llm_service:
            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œå‰µå»ºå¢å¼·æŸ¥è©¢
            if context_text:
                enhanced_question = f"""Based on our previous conversation:
{context_text}

Current question: {question}

Please answer the current question considering the context of our conversation."""
            else:
                enhanced_question = question
            
            # è™•ç†æŸ¥è©¢
            result = llm_service.process_query(
                query=enhanced_question,  # ä½¿ç”¨åŒ…å«ä¸Šä¸‹æ–‡çš„å•é¡Œ
                use_rag=True,
                max_docs=max_results
            )
            
            # å¦‚æœå›ç­”ä¸­æ²’æœ‰è€ƒæ…®ä¸Šä¸‹æ–‡ï¼Œæ‰‹å‹•å¢å¼·
            answer = result.get("answer", "No answer generated")
            
            # æ·»åŠ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›æ‡‰
            if context and "previous" not in answer.lower() and "context" not in answer.lower():
                # æª¢æŸ¥æ˜¯å¦æ‡‰è©²åƒè€ƒä¹‹å‰çš„å°è©±
                if any(word in question.lower() for word in ['it', 'that', 'this', 'more', 'explain', 'detail']):
                    answer = f"Based on our conversation, {answer}"
            
            return {
                "answer": answer,
                "sources": result.get("sources", []),
                "confidence": 0.85,
                "success": result.get("success", False),
                "model": result.get("model", "integrated"),
                "metadata": {
                    "sources_found": len(result.get("sources", [])),
                    "processing_time": "< 1s",
                    "rag_enabled": result.get("use_rag", True),
                    "context_used": len(context),
                    "session_id": session_id,
                    "context_aware": True
                }
            }
        else:
            # Fallback to enhanced mock response with context awareness
            if context_text:
                answer = f"I understand this is a follow-up to our conversation about '{question}'. "
                answer += f"Your RAG system is ready but needs full service integration. "
                answer += f"I can see we have {len(context)} messages in our conversation history."
            else:
                answer = f"I understand you're asking about '{question}'. "
                answer += "Your RAG system is partially initialized but needs full service integration."
            
            return {
                "answer": answer,
                "sources": [
                    {
                        "content": f"Mock relevant content for: {question}",
                        "metadata": {"filename": "system_ready.txt", "page": 1, "score": 0.9}
                    }
                ],
                "confidence": 0.75,
                "success": True,
                "model": "fallback_enhanced_with_memory",
                "metadata": {
                    "context_used": len(context),
                    "session_id": session_id
                }
            }
            
    except Exception as e:
        print(f"âŒ Query processing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Query processing error: {str(e)}")

@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """
    Real document upload and processing
    """
    try:
        # Check file type
        allowed_types = [
            "application/pdf", 
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            "text/plain"
        ]
        if file.content_type not in allowed_types:
            # Check by extension as fallback
            ext = file.filename.lower().split('.')[-1]
            if ext not in ['pdf', 'txt', 'docx', 'doc', 'md']:
                raise HTTPException(status_code=400, detail="Unsupported file format")
        
        # Create storage directory
        storage_dir = "storage"
        os.makedirs(storage_dir, exist_ok=True)
        
        # Save file
        file_path = os.path.join(storage_dir, file.filename)
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        chunks_created = 0
        processing_status = "uploaded"
        
        # Process document if services are available
        if doc_parser and vector_service:
            try:
                # Parse document and create chunks
                print(f"ğŸ“„ Parsing document: {file.filename}")
                
                if chunk_document:
                    chunks = chunk_document(file_path, chunk_size=500, overlap=50)
                else:
                    result = doc_parser.parse_document(file_path)
                    content_text = result.get('content', '')
                    
                    # Create chunks manually
                    chunk_size = 500
                    overlap = 50
                    chunks = []
                    for i in range(0, len(content_text), chunk_size - overlap):
                        chunk_text = content_text[i:i + chunk_size]
                        if chunk_text.strip():
                            chunks.append({
                                "content": chunk_text.strip(),
                                "metadata": {
                                    "filename": file.filename,
                                    "chunk_index": len(chunks),
                                    "source_file": file.filename
                                }
                            })
                
                # Add to vector database
                print(f"ğŸ” Adding {len(chunks)} chunks to vector database")
                vector_service.add_documents(chunks)
                
                chunks_created = len(chunks)
                processing_status = "processed_and_indexed"
                print(f"âœ… Document processed: {chunks_created} chunks created")
                
            except Exception as e:
                print(f"âš ï¸  Document processing error: {e}")
                processing_status = f"uploaded_but_processing_failed: {str(e)}"
        else:
            processing_status = "uploaded_awaiting_service_integration"
        
        return {
            "message": "File uploaded successfully",
            "filename": file.filename,
            "chunks_created": chunks_created,
            "file_size": len(content),
            "processing_status": processing_status,
            "ready_for_queries": chunks_created > 0
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload error: {str(e)}")

@app.post("/upload-batch")
async def upload_batch(files: List[UploadFile] = File(...)):
    """
    Batch document upload and processing
    """
    try:
        results = []
        total_chunks = 0
        
        # Create storage directory
        storage_dir = "storage"
        os.makedirs(storage_dir, exist_ok=True)
        
        for file in files:
            # Check file type
            allowed_types = [
                "application/pdf", 
                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                "text/plain",
                "text/markdown"
            ]
            
            # Check content type or file extension
            is_allowed = file.content_type in allowed_types
            if not is_allowed:
                ext = file.filename.lower().split('.')[-1]
                if ext in ['pdf', 'txt', 'docx', 'doc', 'md']:
                    is_allowed = True
            
            if not is_allowed:
                results.append({
                    "filename": file.filename,
                    "status": "error",
                    "message": "Unsupported file format",
                    "chunks_created": 0
                })
                continue
            
            try:
                # Save file
                file_path = os.path.join(storage_dir, file.filename)
                with open(file_path, "wb") as buffer:
                    content = await file.read()
                    buffer.write(content)
                
                chunks_created = 0
                
                # Process document if services are available
                if doc_parser and vector_service:
                    print(f"ğŸ“„ Processing: {file.filename}")
                    
                    if chunk_document:
                        chunks = chunk_document(file_path, chunk_size=500, overlap=50)
                    else:
                        result = doc_parser.parse_document(file_path)
                        content_text = result.get('content', '')
                        
                        # Create chunks manually
                        chunk_size = 500
                        overlap = 50
                        chunks = []
                        for i in range(0, len(content_text), chunk_size - overlap):
                            chunk_text = content_text[i:i + chunk_size]
                            if chunk_text.strip():
                                chunks.append({
                                    "content": chunk_text.strip(),
                                    "metadata": {
                                        "filename": file.filename,
                                        "chunk_index": len(chunks),
                                        "source_file": file.filename
                                    }
                                })
                    
                    # Add to vector database
                    vector_service.add_documents(chunks)
                    chunks_created = len(chunks)
                    total_chunks += chunks_created
                    print(f"âœ… {file.filename}: {chunks_created} chunks created")
                
                results.append({
                    "filename": file.filename,
                    "status": "success",
                    "message": "File processed successfully",
                    "chunks_created": chunks_created,
                    "file_size": len(content)
                })
                
            except Exception as e:
                print(f"âŒ Error processing {file.filename}: {e}")
                results.append({
                    "filename": file.filename,
                    "status": "error",
                    "message": str(e),
                    "chunks_created": 0
                })
        
        success_count = len([r for r in results if r['status'] == 'success'])
        print(f"ğŸ‰ Batch upload complete: {success_count}/{len(files)} files processed, {total_chunks} total chunks")
        
        return {
            "message": f"Batch upload completed: {success_count}/{len(files)} files processed",
            "total_chunks": total_chunks,
            "results": results
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Batch upload error: {str(e)}")

@app.get("/documents")
async def list_documents():
    """
    List uploaded documents with processing status
    """
    try:
        documents = []
        storage_dirs = ["storage", "data"]
        
        doc_id = 0
        for storage_dir in storage_dirs:
            if os.path.exists(storage_dir):
                for filename in os.listdir(storage_dir):
                    if filename.lower().endswith(('.pdf', '.docx', '.txt')):
                        file_path = os.path.join(storage_dir, filename)
                        file_size = os.path.getsize(file_path)
                        
                        # Try to get chunk info from vector service
                        chunks = 0
                        if vector_service:
                            try:
                                # This would need to be implemented in your vector service
                                # chunks = vector_service.get_chunks_for_file(filename)
                                chunks = 5  # Mock for now
                            except:
                                chunks = 0
                        
                        documents.append({
                            "id": f"doc_{doc_id}",
                            "filename": filename,
                            "chunks": chunks,
                            "file_size": file_size,
                            "status": "indexed" if chunks > 0 else "uploaded",
                            "storage_location": storage_dir
                        })
                        doc_id += 1
        
        return documents
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing documents: {str(e)}")

@app.delete("/documents/{doc_id}")
async def delete_document(doc_id: str):
    """
    Delete document from storage and vector database
    """
    try:
        # TODO: Implement real document deletion
        return {
            "message": "Document deletion functionality ready for implementation",
            "doc_id": doc_id,
            "status": "mock_deleted"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/stats")
async def get_detailed_stats():
    """
    Get detailed system statistics
    """
    try:
        stats = {
            "system": {
                "version": "2.1.0",
                "status": "operational",
                "features": ["conversation_memory", "context_aware_responses"]
            },
            "services": {
                "llm_service": llm_service is not None,
                "vector_service": vector_service is not None,
                "document_parser": doc_parser is not None
            },
            "storage": {
                "total_documents": len(await list_documents()),
                "total_size_mb": 0
            }
        }
        
        if llm_service:
            service_status = llm_service.get_service_status()
            stats["llm"] = service_status
        
        if vector_service:
            try:
                vector_stats = vector_service.get_stats()
                stats["vectors"] = vector_stats
            except:
                stats["vectors"] = {"status": "unavailable"}
        
        return stats
        
    except Exception as e:
        return {"error": str(e)}

# New endpoint for conversation management
@app.post("/api/conversation/clear")
async def clear_conversation(session_id: str = "default"):
    """Clear conversation history for a session"""
    return {
        "message": f"Conversation cleared for session {session_id}",
        "session_id": session_id,
        "status": "cleared"
    }

@app.get("/api/conversation/export/{session_id}")
async def export_conversation(session_id: str = "default"):
    """Export conversation history"""
    return {
        "session_id": session_id,
        "messages": [],  # Would be populated from a database in production
        "exported_at": "2024-01-01T00:00:00Z"
    }

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Starting KnowledgeOps Platform with Conversation Memory...")
    print("ğŸ’­ New Feature: Context-aware responses based on conversation history")
    uvicorn.run(app, host="0.0.0.0", port=8000)
